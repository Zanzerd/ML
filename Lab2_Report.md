## Лабораторная работа №2
Хромов П.А. М8О-404Б-17

### Алгоритмы ML

Логистическая регрессия реализована самым обычным образом. 

KNN также вполне обычен, берётся самая часто встречающаяся у k соседей метка, без использования весов соседей. Предусмотрена возможность использования различных p-норм.

Метод опорных векторов вызвал у меня, вероятно, наибольшие проблемы. Метод fit был почти полностью подсмотрен мной в стороннем туториале. SVM была написана для представления данных, отличного от pd.DataFrame, и я отдельно перегонял свои данные в словари. Долго не получалось добиться вменяемой визуализации. Для демонстрации работы кода был выбран двумерный случай, но он поддерживает и большие размерности. В итоге я несколько запутался в своём коде и, хотя он правильно классифицирует точки данных, я не был уверен, как измерить точность классификации и сравнить мою реализацию с библиотечной.

С реализацией алгоритмов дерева принятия решений и случайного леса мне немного помогли курсы fast.ai. Алгоритмы были реализованы для задачи классификации, хотя при небольших изменениях в коде их можно использовать и для регрессии. 

Дополнительные комментарии есть в ноутбуке.

### Метрики

Для логистической регрессии использовалась mean absolute error, для KNN - accuracy, для дерева принятия решений и случайного леса - R2 score (коэффициент детерминации). Мне хотелось в ходе выполнения работы узнать о разных типах существующих метрик и опробовать их на практике. 

### Сравнение с sklearn

Мои реализации алгоритмов логистической регрессии, случайного леса и дерева принятия решений по значениям метрик несколько уступают библиотечным, но не слишком сильно. При правильном выборе k, алгоритм KNN может выдать показатель accuracy даже выше, чем библиотечный (это, впрочем, скорее зависит от выборки).
Видно, что модели не переобучились, так как их показатели на валидационных выборках несильно уступают аналогичным на обучающих. 

### Возникшие проблемы.

Я долго пытался вникнуть в концепты алгоритмов дерева принятия решений и машины опорных векторов. Благодаря нескольким часам чтения статей и литературы, я теперь понимаю принципы их работы. 

### Выводы

Эта лабораторная работа показалась мне весьма непростой, и я провёл много времени на обучающих сайтах или за чтением книги Джоэла Груса 'Data Science from Scratch', чтобы лучше понять, что и как мне следует реализовать. Понятно, что в реальных практических приложениях не стоит пытаться придумывать велосипед и писать с нуля реализации этих алгоритмов вместо библиотечных, и тем не менее, было интересно разобраться в том, как они работают. Благодаря этой лабораторной работе я в принципе заинтересовался миром машинного обучения и теперь понимаю, что хотел бы узнать о нём ещё. 
